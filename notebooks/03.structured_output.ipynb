{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure your environmental variables and dependencies are ready to use LLM services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In langrila, the module to chat with LLM and the module to call tools are completely separated. FunctionalChat class is the combination of those two.\n",
    "\n",
    "- `XXXChatModule`: Only focuses on doing conversation with LLM\n",
    "- `XXXFunctionCallingModule`: Only focuses on calling tools\n",
    "- `XXXFunctionalChat`: The combination of the two. FunctionCallingModule works at first and then ChatModule performs. If any tool is not provided, this module behaves as just ChatModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila.gemini import GeminiFunctionalChat\n",
    "from langrila.openai import OpenAIFunctionalChat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Chat Completion and Gemini support json mode generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For OpenAI Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = OpenAIFunctionalChat(\n",
    "    api_key_env_name=\"API_KEY\",\n",
    "    model_name=\"gpt-4o-2024-05-13\",\n",
    "    api_type=\"openai\",\n",
    "    json_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"# INSTRUCTION\n",
    "Extract fruit names from the following text and output with the following JSON format. Don't generate JSON code block.\n",
    "\n",
    "# TEXT\n",
    "{prompt}\n",
    "\n",
    "# OUTPUT JSON FORMAT\n",
    "{{\n",
    "    \"fruits\": [\"xxx\", \"xxx\", \"xxx\"]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fruits': ['apples', 'oranges', 'bananas']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(prompt=\"This store sells apples, oranges, and bananas.\")\n",
    "response = chat_openai.run(prompt=prompt)\n",
    "json.loads(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pydantic schema is also available for latest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = OpenAIFunctionalChat(\n",
    "    api_key_env_name=\"API_KEY\",\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    api_type=\"openai\",\n",
    "    json_mode=True,  # also needed\n",
    "    response_schema=MathReasoning,\n",
    "    system_instruction=\"You are a helpful math tutor. Guide the user through the solution step by step.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathReasoning(steps=[Step(explanation='The first step to solving this equation is to isolate the variable term (8x) on one side of the equation. To do this, we need to get rid of the constant term (+7) on the left side. We can subtract 7 from both sides of the equation to achieve this.', output='8x + 7 - 7 = -23 - 7'), Step(explanation='Subtracting 7 from both sides results in the cancellation of the +7 on the left side, leaving us only with the variable term on the left.', output='8x = -30'), Step(explanation='Now that we have 8x on the left side, we need to solve for x by getting x by itself. Since x is currently multiplied by 8, we can divide both sides by 8 to isolate x.', output='8x / 8 = -30 / 8'), Step(explanation='Dividing both sides by 8 simplifies the equation. The variable x is now isolated on the left side, and we perform the division on the right side.', output='x = -30 / 8'), Step(explanation='-30 divided by 8 simplifies to -3.75. Thus, the value of x is -3.75.', output='x = -3.75')], final_answer='x = -3.75')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"how can I solve 8x + 7 = -23\"\n",
    "response = chat_openai.run(prompt=prompt)\n",
    "\n",
    "# resposne message is in JSON string that can be converted to specified schema\n",
    "resposne_json = json.loads(response.message.content[0].text)\n",
    "MathReasoning(**resposne_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class ResponseSchema(TypedDict):\n",
    "    fruits: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = GeminiFunctionalChat(\n",
    "    api_key_env_name=\"GEMINI_API_KEY\",\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    json_mode=True,\n",
    "    response_schema=ResponseSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"This store sells apples, oranges, and bananas.\"\n",
    "\n",
    "response = gemini.run(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fruits': ['apples', 'oranges', 'bananas']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
